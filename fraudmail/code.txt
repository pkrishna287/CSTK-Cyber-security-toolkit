!pip install keras==2.2.4 # critical dependency
!pip install -q bert-tensorflow==1.0.1
!pip freeze > kaggle_image_requirements.txt
# Import neural network libraries
import tensorflow as tf
import tensorflow_hub as hub
from bert.tokenization import FullTokenizer
from tensorflow.keras import backend as K

# Initialize session
sess = tf.Session()
import os
import re
import pandas as pd
import numpy as np
from tqdm import tqdm
Nsamp = 2000 # number of samples to generate in each class - 'spam', 'not spam'
maxtokens = 300 # the maximum number of tokens per document
maxtokenlen = 300 # th
def tokenize(row):
    if row is None or row is '':
        tokens = ""
    else:
        try:
            tokens = row.split(" ")[:maxtokens]
        except:
            tokens=""
    return tokens
def reg_expressions(row):
    tokens = []
    try:
        for token in row:
            token = token.lower()
            token = re.sub(r'[\W\d]', "", token)
            token = token[:maxtokenlen] # truncate token
            tokens.append(token)
    except:
        token = ""
        tokens.append(token)
    return tokens
import nltk

nltk.download('stopwords')
from nltk.corpus import stopwords
stopwords = stopwords.words('english')    
print(stopwords) # see default stopwords

def stop_word_removal(row):
    token = [token for token in row if token not in stopwords]
    token = filter(None, token)
    return token
filepath = "../input/enron-email-dataset/emails.csv"

# Read the data into a pandas dataframe called emails
emails = pd.read_csv(filepath)

print("Successfully loaded {} rows and {} columns!".format(emails.shape[0], emails.shape[1]))
print(emails.head())
print(emails.loc[0]["message"])
import email

def extract_messages(df):
    messages = []
    for item in df["message"]:
        # Return a message object structure from a string
        e = email.message_from_string(item)    
        # get message body  
        message_body = e.get_payload()
        messages.append(message_body)
    print("Successfully retrieved message body from e-mails!")
    return messages

bodies = extract_messages(emails)
import random
bodies_df = pd.DataFrame(random.sample(bodies, 10000))

# expand default pandas display options to make emails more clearly visible when printed
pd.set_option('display.max_colwidth', 300)

bodies_df.head() 
filepath = "../input/fraudulent-email-corpus/fradulent_emails.txt"
with open(filepath, 'r',encoding="latin1") as file:
    data = file.read()
    
# split on a code word appearing close to the beginning of each email
fraud_emails = data.split("From r")

print("Successfully loaded {} spam emails!".format(len(fraud_emails)))
fraud_bodies = extract_messages(pd.DataFrame(fraud_emails,columns=["message"],dtype=str))
fraud_bodies_df = pd.DataFrame(fraud_bodies[1:])

fraud_bodies_df.head()
import random

# Convert everything to lower-case, truncate to maxtokens and truncate each token to maxtokenlen
EnronEmails = bodies_df.iloc[:,0].apply(tokenize)
EnronEmails = EnronEmails.apply(stop_word_removal)
EnronEmails = EnronEmails.apply(reg_expressions)
EnronEmails = EnronEmails.sample(Nsamp)

SpamEmails = fraud_bodies_df.iloc[:,0].apply(tokenize)
SpamEmails = SpamEmails.apply(stop_word_removal)
SpamEmails = SpamEmails.apply(reg_expressions)
SpamEmails = SpamEmails.sample(Nsamp)

raw_data = pd.concat([SpamEmails,EnronEmails], axis=0).values
print("Shape of combined data represented as numpy array is:")
print(raw_data.shape)
print("Data represented as numpy array is:")
print(raw_data)

# corresponding labels
Categories = ['spam','notspam']
header = ([1]*Nsamp)
header.extend(([0]*Nsamp))
def unison_shuffle(a, b):
    p = np.random.permutation(len(b))
    data = a[p]
    header = np.asarray(b)[p]
    return data, header

# function for converting data into the right format, due to the difference in required format from sklearn models
# we expect a single string per email here, versus a list of tokens for the sklearn models previously explored
def convert_data(raw_data,header):
    converted_data, labels = [], []
    for i in range(raw_data.shape[0]):
        out = ' '.join(raw_data[i])
        converted_data.append(out)
        labels.append(header[i])
        #print(i)
    converted_data = np.array(converted_data, dtype=object)[:, np.newaxis]
    
    return converted_data, np.array(labels)

raw_data, header = unison_shuffle(raw_data, header)

# split into independent 70% training and 30% testing sets
idx = int(0.7*raw_data.shape[0])
# 70% of data for training
train_x, train_y = convert_data(raw_data[:idx],header[:idx])
# remaining 30% for testing
test_x, test_y = convert_data(raw_data[idx:],header[idx:])

print("train_x/train_y list details, to make sure it is of the right form:")
print(len(train_x))
print(train_x)
print(train_y[:5])
print(train_y.shape)
class InputExample(object):
    """A single training/test example for simple sequence classification."""

    def __init__(self, guid, text_a, text_b=None, label=None):
        """Constructs a InputExample.
    Args:
      guid: Unique id for the example.
      text_a: string. The untokenized text of the first sequence. For single
        sequence tasks, only this sequence must be specified.
      text_b: (Optional) string. The untokenized text of the second sequence.
        Only must be specified for sequence pair tasks.
      label: (Optional) string. The label of the example. This should be
        specified for train examples, but not for test examples.
    """
        self.guid = guid
        self.text_a = text_a
        self.text_b = text_b
        self.label = label


def create_tokenizer_from_hub_module(bert_path):
    """Get the vocab file and casing info from the Hub module."""
    bert_module = hub.Module(bert_path)
    tokenization_info = bert_module(signature="tokenization_info", as_dict=True)
    vocab_file, do_lower_case = sess.run(
        [tokenization_info["vocab_file"], tokenization_info["do_lower_case"]]
    )

    return FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)


def convert_single_example(tokenizer, example, max_seq_length=256):
    """Converts a single `InputExample` into a single `InputFeatures`."""

    tokens_a = tokenizer.tokenize(example.text_a)
    if len(tokens_a) > max_seq_length - 2:
        tokens_a = tokens_a[0 : (max_seq_length - 2)]

    tokens = []
    segment_ids = []
    tokens.append("[CLS]")
    segment_ids.append(0)
    for token in tokens_a:
        tokens.append(token)
        segment_ids.append(0)
    tokens.append("[SEP]")
    segment_ids.append(0)

    input_ids = tokenizer.convert_tokens_to_ids(tokens)

    # The mask has 1 for real tokens and 0 for padding tokens. Only real
    # tokens are attended to.
    input_mask = [1] * len(input_ids)

    # Zero-pad up to the sequence length.
    while len(input_ids) < max_seq_length:
        input_ids.append(0)
        input_mask.append(0)
        segment_ids.append(0)

    assert len(input_ids) == max_seq_length
    assert len(input_mask) == max_seq_length
    assert len(segment_ids) == max_seq_length

    return input_ids, input_mask, segment_ids, example.label


def convert_examples_to_features(tokenizer, examples, max_seq_length=256):
    """Convert a set of `InputExample`s to a list of `InputFeatures`."""

    input_ids, input_masks, segment_ids, labels = [], [], [], []
    for example in tqdm(examples, desc="Converting examples to features"):
        input_id, input_mask, segment_id, label = convert_single_example(
            tokenizer, example, max_seq_length
        )
        input_ids.append(input_id)
        input_masks.append(input_mask)
        segment_ids.append(segment_id)
        labels.append(label)
    return (
        np.array(input_ids),
        np.array(input_masks),
        np.array(segment_ids),
        np.array(labels).reshape(-1, 1),
    )


def convert_text_to_examples(texts, labels):
    """Create InputExamples"""
    InputExamples = []
    for text, label in zip(texts, labels):
        InputExamples.append(
            InputExample(guid=None, text_a=" ".join(text), text_b=None, label=label)
        )
    return InputExamples
class BertLayer(tf.keras.layers.Layer):
    def __init__(
        self,
        n_fine_tune_layers=10,
        pooling="mean",
        bert_path="https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1",
        **kwargs,
    ):
        self.n_fine_tune_layers = n_fine_tune_layers
        self.trainable = True
        self.output_size = 768
        self.pooling = pooling
        self.bert_path = bert_path
        if self.pooling not in ["first", "mean"]:
            raise NameError(
                f"Undefined pooling type (must be either first or mean, but is {self.pooling}"
            )

        super(BertLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        self.bert = hub.Module(
            self.bert_path, trainable=self.trainable, name=f"{self.name}_module"
        )

        # Remove unused layers
        trainable_vars = self.bert.variables
        if self.pooling == "first":
            trainable_vars = [var for var in trainable_vars if not "/cls/" in var.name]
            trainable_layers = ["pooler/dense"]

        elif self.pooling == "mean":
            trainable_vars = [
                var
                for var in trainable_vars
                if not "/cls/" in var.name and not "/pooler/" in var.name
            ]
            trainable_layers = []
        else:
            raise NameError(
                f"Undefined pooling type (must be either first or mean, but is {self.pooling}"
            )

        # Select how many layers to fine tune
        for i in range(self.n_fine_tune_layers):
            trainable_layers.append(f"encoder/layer_{str(11 - i)}")

        # Update trainable vars to contain only the specified layers
        trainable_vars = [
            var
            for var in trainable_vars
            if any([l in var.name for l in trainable_layers])
        ]

        # Add to trainable weights
        for var in trainable_vars:
            self._trainable_weights.append(var)

        for var in self.bert.variables:
            if var not in self._trainable_weights:
                self._non_trainable_weights.append(var)

        super(BertLayer, self).build(input_shape)

    def call(self, inputs):
        inputs = [K.cast(x, dtype="int32") for x in inputs]
        input_ids, input_mask, segment_ids = inputs
        bert_inputs = dict(
            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids
        )
        if self.pooling == "first":
            pooled = self.bert(inputs=bert_inputs, signature="tokens", as_dict=True)[
                "pooled_output"
            ]
        elif self.pooling == "mean":
            result = self.bert(inputs=bert_inputs, signature="tokens", as_dict=True)[
                "sequence_output"
            ]

            mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)
            masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (
                    tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)
            input_mask = tf.cast(input_mask, tf.float32)
            pooled = masked_reduce_mean(result, input_mask)
        else:
            raise NameError(f"Undefined pooling type (must be either first or mean, but is {self.pooling}")

        return pooled

    def compute_output_shape(self, input_shape):
        return (input_shape[0], self.output_size)
from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional
from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional
def build_model(max_seq_length):
    in_id = tf.keras.layers.Input(shape=(max_seq_length,), name="input_ids")
    in_mask = tf.keras.layers.Input(shape=(max_seq_length,), name="input_masks")
    in_segment = tf.keras.layers.Input(shape=(max_seq_length,), name="segment_ids")
    bert_inputs = [in_id, in_mask, in_segment]
    
    # just extract BERT features, don't fine-tune
    bert_output = BertLayer(n_fine_tune_layers=0)(bert_inputs)
    # train dense classification layer on top of extracted features
    dense = tf.keras.layers.Dense(256, activation="relu")(bert_output)
    pred = tf.keras.layers.Dense(1, activation="sigmoid")(dense)
    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)
    model.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])
    model.summary()

    return model

# Function to initialize variables correctly
def initialize_vars(sess):
    sess.run(tf.local_variables_initializer())
    sess.run(tf.global_variables_initializer())
    sess.run(tf.tables_initializer())
    K.set_session(sess)
bert_path = "https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1" 

# Instantiate tokenizer
tokenizer = create_tokenizer_from_hub_module(bert_path)

# Convert data to InputExample format
train_examples = convert_text_to_examples(train_x, train_y)
test_examples = convert_text_to_examples(test_x, test_y)

# Convert to features
(train_input_ids,train_input_masks,train_segment_ids,train_labels) = \
convert_examples_to_features(tokenizer, train_examples, max_seq_length=maxtokens)
(test_input_ids,test_input_masks,test_segment_ids,test_labels) = \
convert_examples_to_features(tokenizer, test_examples, max_seq_length=maxtokens)

# Build model
model = build_model(maxtokens)

# Instantiate variables
initialize_vars(sess)
history = model.fit([train_input_ids, train_input_masks, train_segment_ids],train_labels,
                    validation_data=([test_input_ids, test_input_masks, test_segment_ids],test_labels),
                    epochs=20,batch_size=32)
import matplotlib.pyplot as plt

df_history = pd.DataFrame(history.history)
fig,ax = plt.subplots()
plt.plot(range(df_history.shape[0]),df_history['val_acc'],'bs--',label='validation')
plt.plot(range(df_history.shape[0]),df_history['acc'],'r^--',label='training')
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.title('BERT Email Classification Training')
plt.legend(loc='best')
plt.grid()
plt.show()

fig.savefig('BERT_Accuracy.eps', format='eps')
fig.savefig('BERT_Accuracy.pdf', format='pdf')
fig.savefig('BERT_Accuracy.png', format='png')
fig.savefig('BERT_Accuracy.svg', format='svg')
import matplotlib.pyplot as plt

df_history = pd.DataFrame(history.history)
fig,ax = plt.subplots()
plt.plot(range(df_history.shape[0]),df_history['val_loss'],'bs--',label='validation')
plt.plot(range(df_history.shape[0]),df_history['loss'],'r^--',label='training')
plt.xlabel('epoch')
plt.ylabel('loss')
plt.title('BERT Email Classification Training')
plt.legend(loc='best')
plt.grid()
plt.show()

fig.savefig('BERT_loss.eps', format='eps')
fig.savefig('BERT_loss.pdf', format='pdf')
fig.savefig('BERT_loss.png', format='png')
fig.savefig('BERT_loss.svg', format='svg')
from IPython.display import HTML
def create_download_link(title = "Download file", filename = "data.csv"):  
    html = '<a href={filename}>{title}</a>'
    html = html.format(title=title,filename=filename)
    return HTML(html)

create_download_link(filename='BERT_loss.eps')
create_download_link(filename='BERT_loss.pdf')
create_download_link(filename='BERT_loss.png')
create_download_link(filename='BERT_loss.svg')
from IPython.display import HTML
def create_download_link(title = "Download file", filename = "data1.csv"):  
    html = '<a href={filename}>{title}</a>'
    html = html.format(title=title,filename=filename)
    return HTML(html)

create_download_link(filename='BERT_Accuracy.eps')
create_download_link(filename='BERT_Accuracy.pdf')
create_download_link(filename='BERT_Accuracy.png')
create_download_link(filename='BERT_Accuracy.svg')
from tensorflow.keras.layers import concatenate
from tensorflow.keras.models import Model 
print('Getting training features and concatenation---------------start')
model1= Model(model.input, model.layers[-2].output)
model1.summary()
model1_features=model1.predict([train_input_ids, train_input_masks, train_segment_ids])
model1_features=pd.DataFrame(model1_features)
print("Branch1_features", model1_features.shape)
model1_features_test=model1.predict([test_input_ids, test_input_masks, test_segment_ids])
model1_features_test=pd.DataFrame(model1_features_test)
print("Branch1_features_Test", model1_features_test.shape)
feature_train_labels=pd.DataFrame(train_labels)
print(feature_train_labels)
model1_features.shape
import tensorflow as tf
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model

# Define input shape
input_shape = (256,)

# Define encoding dimension Feature Reduction Converting 256 to 50 Only.
encoding_dim = 50

# Define input layer
input_layer = Input(shape=input_shape)

# Define encoding layer
encoding_layer = Dense(encoding_dim, activation='relu')(input_layer)

# Define decoding layer
decoding_layer = Dense(input_shape[0], activation='sigmoid')(encoding_layer)

# Define autoencoder model
autoencoder = Model(inputs=input_layer, outputs=decoding_layer)

# Compile autoencoder model
autoencoder.compile(optimizer='adam', loss='binary_crossentropy')

# Print autoencoder summary
autoencoder.summary()
history_encod = model.fit([train_input_ids, train_input_masks, train_segment_ids],train_labels,
                    validation_data=([test_input_ids, test_input_masks, test_segment_ids],test_labels),
                    epochs=5,batch_size=32)
import matplotlib.pyplot as plt

df_history = pd.DataFrame(history_encod.history)
fig,ax = plt.subplots()
plt.plot(range(df_history.shape[0]),df_history['val_acc'],'bs--',label='validation')
plt.plot(range(df_history.shape[0]),df_history['acc'],'r^--',label='training')
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.title('BERT Email Classification Training')
plt.legend(loc='best')
plt.grid()
plt.show()

fig.savefig('Encoder_Accuracy.eps', format='eps')
fig.savefig('Encoder_Accuracy.pdf', format='pdf')
fig.savefig('Encoder_Accuracy.png', format='png')
fig.savefig('Encoder_Accuracy.svg', format='svg')

import matplotlib.pyplot as plt

df_history = pd.DataFrame(history_encod.history)
fig,ax = plt.subplots()
plt.plot(range(df_history.shape[0]),df_history['val_loss'],'bs--',label='validation')
plt.plot(range(df_history.shape[0]),df_history['loss'],'r^--',label='training')
plt.xlabel('epoch')
plt.ylabel('Loss')
plt.title('BERT Email Classification Loss')
plt.legend(loc='best')
plt.grid()
plt.show()

fig.savefig('BERT_loss.eps', format='eps')
fig.savefig('BERT_loss.pdf', format='pdf')
fig.savefig('BERT_loss.png', format='png')
fig.savefig('BERT_loss.svg', format='svg')
encoder = Model(inputs=input_layer, outputs=encoding_layer)

# Print encoder summary
encoder.summary()
encoded_data = encoder.predict(model1_features)

encoded_data.shape
encoded_data_test = encoder.predict(model1_features_test)
encoded_data_test
from sklearn.multiclass import OneVsRestClassifier
from xgboost import XGBClassifier
clf = OneVsRestClassifier(XGBClassifier(max_depth=6,objective='multi:softmax', num_class=2))
clf.fit(encoded_data,feature_train_labels)
pred_XGb1=clf.predict(encoded_data_test)
print("pred_XBG1",pred_XGb1.shape)
from sklearn import metrics
accuracy=metrics.accuracy_score(pred_XGb1,test_labels)
print("Accuracy of combined model with RF: {00:000.4f}".format(accuracy*100))

from sklearn.metrics import f1_score
f1score=f1_score(pred_XGb1,test_labels, average='macro')
print("F1score of combined model with RF: {00:000.4f}".format( f1score*100))

from sklearn.metrics import recall_score
recall = recall_score(test_labels,pred_XGb1, average='macro')
print('Recall score of combined model with RF: {00:000.4f}'.format(recall*100))

from sklearn.metrics import precision_score
precision = precision_score(pred_XGb1.round(),test_labels,average='macro')
print('Precision of combined model with RF: {000:000.4f}'.format(precision*100))
from sklearn.metrics import classification_report
labe=['Legtimate', 'fraudulent']
print(classification_report(pred_XGb1, test_labels, target_names=labe))
from sklearn.metrics import roc_auc_score
#Generate class membership probabilities
ROC_auc_score=roc_auc_score( test_labels,pred_XGb1, average="weighted")
print("ROC_auc_score of XGBooster is ", ROC_auc_score)
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import multilabel_confusion_matrix
cmxgb = multilabel_confusion_matrix(pred_XGb1, test_labels)
print(cmxgb)
from sklearn.metrics import multilabel_confusion_matrix
cm = multilabel_confusion_matrix(test_labels,pred_XGb1)
print(cm)
first_cat=cm[0]
tn, fp, fn, tp = first_cat.ravel()
fpr = fp / (tn + fp)
fnr = fn / (tp + fn)
TPR = tp/(tp+fn)
TNR = tn/(tn+fp)
acc_1=(tp+tn)/(tp+tn+fp+fn)
print("fpr for Legtimate is ",fpr)
print("fnr for Legtimate is ",fnr)
print("TPR for Legtimate is ",TPR)
print("TNR for Legtimate is ",TNR)
print("Accuracy for Legtimate is ",acc_1*100)

second_cat=cm[1]
tn, fp, fn, tp = second_cat.ravel()
fpr = fp / (tn + fp)
fnr = fn / (tp + fn)
TPR = tp/(tp+fn)
TNR = tn/(tn+fp)
acc_2=(tp+tn)/(tp+tn+fp+fn)
print("fpr for fraudulent is ",fpr)
print("fnr for fraudulent is ",fnr)
print("TPR for fraudulent is ",TPR)
print("TNR for fraudulent is ",TNR)
print("Accuracy for fraudulent is ",acc_2*100)
